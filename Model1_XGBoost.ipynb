{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import decomposition, ensemble\n",
    "import pandas, xgboost\n",
    "\n",
    "#from keras.preprocessing import text, sequence\n",
    "#from keras import layers, models, optimizers\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "\n",
    "train_text = pd.read_json(\"test_instances.json\", lines=True)\n",
    "train_truth = pd.read_json(\"test_truth.json\", lines=True)\n",
    "test_text = pd.read_json(\"train_instances.json\", lines=True)\n",
    "test_truth = pd.read_json(\"train_truth.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>postMedia</th>\n",
       "      <th>postText</th>\n",
       "      <th>postTimestamp</th>\n",
       "      <th>targetCaptions</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>targetKeywords</th>\n",
       "      <th>targetParagraphs</th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>truthClass</th>\n",
       "      <th>truthJudgments</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>truthMedian</th>\n",
       "      <th>truthMode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>608310377143799808</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Apple's iOS 9 'App thinning' feature will giv...</td>\n",
       "      <td>Tue Jun 09 16:31:10 +0000 2015</td>\n",
       "      <td>['App thinning' will be supported on Apple's i...</td>\n",
       "      <td>'App thinning' will be supported on Apple's iO...</td>\n",
       "      <td>Apple,gives,gigabytes,iOS,9,app,thinning,featu...</td>\n",
       "      <td>[Paying for a 64GB phone only to discover that...</td>\n",
       "      <td>Apple gives back gigabytes: iOS 9 'app thinnin...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[0.0, 0.6666667, 0.0, 0.33333334000000003, 0.0]</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>609297109095972864</td>\n",
       "      <td>[media/609297109095972864.jpg]</td>\n",
       "      <td>[RT @kenbrown12: Emerging market investors are...</td>\n",
       "      <td>Fri Jun 12 09:52:05 +0000 2015</td>\n",
       "      <td>[Stocks Fall as Investors Watch Central Banks,...</td>\n",
       "      <td>Global investors have yanked $9.3 billion from...</td>\n",
       "      <td>emerging market,emerging markets,em flows,em i...</td>\n",
       "      <td>[Emerging markets are out of favor., Global in...</td>\n",
       "      <td>Emerging Markets Suffer Largest Outflow in Sev...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[0.6666667, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>609504474621612032</td>\n",
       "      <td>[]</td>\n",
       "      <td>[U.S. Soccer should start answering tough ques...</td>\n",
       "      <td>Fri Jun 12 23:36:05 +0000 2015</td>\n",
       "      <td>[US to vote for Ali in FIFA election and not B...</td>\n",
       "      <td>A U.S. Senator's scathing letter questioned U....</td>\n",
       "      <td></td>\n",
       "      <td>[WINNIPEG, Manitoba – The bubble U.S. Soccer i...</td>\n",
       "      <td>U.S. Soccer should start answering tough quest...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>[0.33333334000000003, 0.6666667, 1.0, 0.0, 0.6...</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>609748367049105408</td>\n",
       "      <td>[]</td>\n",
       "      <td>[How theme parks like Disney World left the mi...</td>\n",
       "      <td>Sat Jun 13 15:45:13 +0000 2015</td>\n",
       "      <td>[Some 1,000 persons turned out in Albuquerque,...</td>\n",
       "      <td>America's top family vacation spots, like the ...</td>\n",
       "      <td>disney, disney world, disney ticket prices, di...</td>\n",
       "      <td>[When Walt Disney World opened in an Orlando s...</td>\n",
       "      <td>How theme parks like Disney World left the mid...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[1.0, 0.0, 0.33333334000000003, 0.333333340000...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608688782821453824</td>\n",
       "      <td>[media/608688782821453825.jpg]</td>\n",
       "      <td>[Could light bulbs hurt your health? One compa...</td>\n",
       "      <td>Wed Jun 10 17:34:49 +0000 2015</td>\n",
       "      <td>[Electric lights have made the world safer and...</td>\n",
       "      <td>One company will put a health notice on all th...</td>\n",
       "      <td>health, Should there be warning labels on your...</td>\n",
       "      <td>[(CNN)The light bulb always makes the world's ...</td>\n",
       "      <td>Warning labels on your light bulbs</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>[1.0, 0.33333334000000003, 0.6666667, 0.333333...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                       postMedia  \\\n",
       "0  608310377143799808                              []   \n",
       "1  609297109095972864  [media/609297109095972864.jpg]   \n",
       "2  609504474621612032                              []   \n",
       "3  609748367049105408                              []   \n",
       "4  608688782821453824  [media/608688782821453825.jpg]   \n",
       "\n",
       "                                            postText  \\\n",
       "0  [Apple's iOS 9 'App thinning' feature will giv...   \n",
       "1  [RT @kenbrown12: Emerging market investors are...   \n",
       "2  [U.S. Soccer should start answering tough ques...   \n",
       "3  [How theme parks like Disney World left the mi...   \n",
       "4  [Could light bulbs hurt your health? One compa...   \n",
       "\n",
       "                    postTimestamp  \\\n",
       "0  Tue Jun 09 16:31:10 +0000 2015   \n",
       "1  Fri Jun 12 09:52:05 +0000 2015   \n",
       "2  Fri Jun 12 23:36:05 +0000 2015   \n",
       "3  Sat Jun 13 15:45:13 +0000 2015   \n",
       "4  Wed Jun 10 17:34:49 +0000 2015   \n",
       "\n",
       "                                      targetCaptions  \\\n",
       "0  ['App thinning' will be supported on Apple's i...   \n",
       "1  [Stocks Fall as Investors Watch Central Banks,...   \n",
       "2  [US to vote for Ali in FIFA election and not B...   \n",
       "3  [Some 1,000 persons turned out in Albuquerque,...   \n",
       "4  [Electric lights have made the world safer and...   \n",
       "\n",
       "                                   targetDescription  \\\n",
       "0  'App thinning' will be supported on Apple's iO...   \n",
       "1  Global investors have yanked $9.3 billion from...   \n",
       "2  A U.S. Senator's scathing letter questioned U....   \n",
       "3  America's top family vacation spots, like the ...   \n",
       "4  One company will put a health notice on all th...   \n",
       "\n",
       "                                      targetKeywords  \\\n",
       "0  Apple,gives,gigabytes,iOS,9,app,thinning,featu...   \n",
       "1  emerging market,emerging markets,em flows,em i...   \n",
       "2                                                      \n",
       "3  disney, disney world, disney ticket prices, di...   \n",
       "4  health, Should there be warning labels on your...   \n",
       "\n",
       "                                    targetParagraphs  \\\n",
       "0  [Paying for a 64GB phone only to discover that...   \n",
       "1  [Emerging markets are out of favor., Global in...   \n",
       "2  [WINNIPEG, Manitoba – The bubble U.S. Soccer i...   \n",
       "3  [When Walt Disney World opened in an Orlando s...   \n",
       "4  [(CNN)The light bulb always makes the world's ...   \n",
       "\n",
       "                                         targetTitle    truthClass  \\\n",
       "0  Apple gives back gigabytes: iOS 9 'app thinnin...  no-clickbait   \n",
       "1  Emerging Markets Suffer Largest Outflow in Sev...  no-clickbait   \n",
       "2  U.S. Soccer should start answering tough quest...     clickbait   \n",
       "3  How theme parks like Disney World left the mid...  no-clickbait   \n",
       "4                 Warning labels on your light bulbs     clickbait   \n",
       "\n",
       "                                      truthJudgments  truthMean  truthMedian  \\\n",
       "0    [0.0, 0.6666667, 0.0, 0.33333334000000003, 0.0]   0.200000     0.000000   \n",
       "1                    [0.6666667, 0.0, 0.0, 0.0, 0.0]   0.133333     0.000000   \n",
       "2  [0.33333334000000003, 0.6666667, 1.0, 0.0, 0.6...   0.533333     0.666667   \n",
       "3  [1.0, 0.0, 0.33333334000000003, 0.333333340000...   0.466667     0.333333   \n",
       "4  [1.0, 0.33333334000000003, 0.6666667, 0.333333...   0.666667     0.666667   \n",
       "\n",
       "   truthMode  \n",
       "0   0.000000  \n",
       "1   0.000000  \n",
       "2   0.666667  \n",
       "3   0.333333  \n",
       "4   1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.merge(train_text,train_truth, on = 'id')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>postMedia</th>\n",
       "      <th>postText</th>\n",
       "      <th>postTimestamp</th>\n",
       "      <th>targetCaptions</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>targetKeywords</th>\n",
       "      <th>targetParagraphs</th>\n",
       "      <th>targetTitle</th>\n",
       "      <th>truthClass</th>\n",
       "      <th>truthJudgments</th>\n",
       "      <th>truthMean</th>\n",
       "      <th>truthMedian</th>\n",
       "      <th>truthMode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>858462320779026432</td>\n",
       "      <td>[]</td>\n",
       "      <td>[UK’s response to modern slavery leaving victi...</td>\n",
       "      <td>Sat Apr 29 23:25:41 +0000 2017</td>\n",
       "      <td>[modern-slavery-rex.jpg]</td>\n",
       "      <td>“Inexcusable” failures in the UK’s system for ...</td>\n",
       "      <td>modern slavery, Department For Work And Pensio...</td>\n",
       "      <td>[Thousands of modern slavery victims have not ...</td>\n",
       "      <td>‘Inexcusable’ failures in UK’s response to mod...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[0.33333333330000003, 0.0, 0.33333333330000003...</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>858421020331560960</td>\n",
       "      <td>[]</td>\n",
       "      <td>[this is good]</td>\n",
       "      <td>Sat Apr 29 20:41:34 +0000 2017</td>\n",
       "      <td>[In this July 1, 2010 file photo, Dr. Charmain...</td>\n",
       "      <td>President Donald Trump has appointed pro-life ...</td>\n",
       "      <td>Americans United for Life, Dr. Charmaine Yoest...</td>\n",
       "      <td>[President Donald Trump has appointed the pro-...</td>\n",
       "      <td>Donald Trump Appoints Pro-Life Advocate as Ass...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>858368123753435136</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The \"forgotten\" Trump roast: Relive his bruta...</td>\n",
       "      <td>Sat Apr 29 17:11:23 +0000 2017</td>\n",
       "      <td>[President Trump will not attend this year's W...</td>\n",
       "      <td>President Trump won't be at this year's White ...</td>\n",
       "      <td>trump whcd, whcd, white house correspondents d...</td>\n",
       "      <td>[When the White House correspondents’ dinner i...</td>\n",
       "      <td>The ‘forgotten’ Trump roast: Relive his brutal...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[0.33333333330000003, 1.0, 0.33333333330000003...</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>858323428260139008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Meet the happiest #dog in the world!]</td>\n",
       "      <td>Sat Apr 29 14:13:46 +0000 2017</td>\n",
       "      <td>[Maru , Maru, Maru, Maru, Maru]</td>\n",
       "      <td>The article is about Maru, a husky dog who has...</td>\n",
       "      <td>Maru, husky, dogs, pandas, furball, instagram</td>\n",
       "      <td>[Adorable is probably an understatement. This ...</td>\n",
       "      <td>Meet The Happiest Dog In The World, Maru The H...</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>[1.0, 0.6666666666000001, 1.0, 1.0, 1.0]</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>858283602626347008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Tokyo's subway is shut down amid fears over a...</td>\n",
       "      <td>Sat Apr 29 11:35:31 +0000 2017</td>\n",
       "      <td>[All nine lines of Tokyo's subway system were ...</td>\n",
       "      <td>The temporary suspension, which lasted ten min...</td>\n",
       "      <td>Tokyo,subway,shut,fears,North,Korean,attack</td>\n",
       "      <td>[One of Tokyo's major subways systems says it ...</td>\n",
       "      <td>Tokyo's subway is shut down amid fears over an...</td>\n",
       "      <td>no-clickbait</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id postMedia  \\\n",
       "0  858462320779026432        []   \n",
       "1  858421020331560960        []   \n",
       "2  858368123753435136        []   \n",
       "3  858323428260139008        []   \n",
       "4  858283602626347008        []   \n",
       "\n",
       "                                            postText  \\\n",
       "0  [UK’s response to modern slavery leaving victi...   \n",
       "1                                     [this is good]   \n",
       "2  [The \"forgotten\" Trump roast: Relive his bruta...   \n",
       "3             [Meet the happiest #dog in the world!]   \n",
       "4  [Tokyo's subway is shut down amid fears over a...   \n",
       "\n",
       "                    postTimestamp  \\\n",
       "0  Sat Apr 29 23:25:41 +0000 2017   \n",
       "1  Sat Apr 29 20:41:34 +0000 2017   \n",
       "2  Sat Apr 29 17:11:23 +0000 2017   \n",
       "3  Sat Apr 29 14:13:46 +0000 2017   \n",
       "4  Sat Apr 29 11:35:31 +0000 2017   \n",
       "\n",
       "                                      targetCaptions  \\\n",
       "0                           [modern-slavery-rex.jpg]   \n",
       "1  [In this July 1, 2010 file photo, Dr. Charmain...   \n",
       "2  [President Trump will not attend this year's W...   \n",
       "3                    [Maru , Maru, Maru, Maru, Maru]   \n",
       "4  [All nine lines of Tokyo's subway system were ...   \n",
       "\n",
       "                                   targetDescription  \\\n",
       "0  “Inexcusable” failures in the UK’s system for ...   \n",
       "1  President Donald Trump has appointed pro-life ...   \n",
       "2  President Trump won't be at this year's White ...   \n",
       "3  The article is about Maru, a husky dog who has...   \n",
       "4  The temporary suspension, which lasted ten min...   \n",
       "\n",
       "                                      targetKeywords  \\\n",
       "0  modern slavery, Department For Work And Pensio...   \n",
       "1  Americans United for Life, Dr. Charmaine Yoest...   \n",
       "2  trump whcd, whcd, white house correspondents d...   \n",
       "3      Maru, husky, dogs, pandas, furball, instagram   \n",
       "4        Tokyo,subway,shut,fears,North,Korean,attack   \n",
       "\n",
       "                                    targetParagraphs  \\\n",
       "0  [Thousands of modern slavery victims have not ...   \n",
       "1  [President Donald Trump has appointed the pro-...   \n",
       "2  [When the White House correspondents’ dinner i...   \n",
       "3  [Adorable is probably an understatement. This ...   \n",
       "4  [One of Tokyo's major subways systems says it ...   \n",
       "\n",
       "                                         targetTitle    truthClass  \\\n",
       "0  ‘Inexcusable’ failures in UK’s response to mod...  no-clickbait   \n",
       "1  Donald Trump Appoints Pro-Life Advocate as Ass...     clickbait   \n",
       "2  The ‘forgotten’ Trump roast: Relive his brutal...  no-clickbait   \n",
       "3  Meet The Happiest Dog In The World, Maru The H...     clickbait   \n",
       "4  Tokyo's subway is shut down amid fears over an...  no-clickbait   \n",
       "\n",
       "                                      truthJudgments  truthMean  truthMedian  \\\n",
       "0  [0.33333333330000003, 0.0, 0.33333333330000003...   0.133333     0.000000   \n",
       "1                          [1.0, 1.0, 1.0, 1.0, 1.0]   1.000000     1.000000   \n",
       "2  [0.33333333330000003, 1.0, 0.33333333330000003...   0.466667     0.333333   \n",
       "3           [1.0, 0.6666666666000001, 1.0, 1.0, 1.0]   0.933333     1.000000   \n",
       "4                          [0.0, 0.0, 0.0, 0.0, 0.0]   0.000000     0.000000   \n",
       "\n",
       "   truthMode  \n",
       "0   0.000000  \n",
       "1   1.000000  \n",
       "2   0.333333  \n",
       "3   1.000000  \n",
       "4   0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.merge(test_text,test_truth, on = 'id')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['postText','truthClass']]\n",
    "test = test[['postText','truthClass']]\n",
    "train['postText']=train['postText'].astype(str).str.replace('\\[|\\]|\\'|\\\"', '')\n",
    "test['postText']=test['postText'].astype(str).str.replace('\\[|\\]|\\'|\\\"', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apples iOS 9 App thinning feature will give yo...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @kenbrown12: Emerging market investors are ...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Soccer should start answering tough quest...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How theme parks like Disney World left the mid...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Could light bulbs hurt your health? One compan...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            postText    truthClass\n",
       "0  Apples iOS 9 App thinning feature will give yo...  no-clickbait\n",
       "1  RT @kenbrown12: Emerging market investors are ...  no-clickbait\n",
       "2  U.S. Soccer should start answering tough quest...     clickbait\n",
       "3  How theme parks like Disney World left the mid...  no-clickbait\n",
       "4  Could light bulbs hurt your health? One compan...     clickbait"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stop words is not working since \n",
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train['truthClass'] = encoder.fit_transform(train['truthClass'])\n",
    "test['truthClass'] = encoder.fit_transform(test['truthClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postText</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK’s response to modern slavery leaving victim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this is good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The forgotten Trump roast: Relive his brutal 2...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meet the happiest #dog in the world!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tokyos subway is shut down amid fears over an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            postText  truthClass\n",
       "0  UK’s response to modern slavery leaving victim...           1\n",
       "1                                       this is good           0\n",
       "2  The forgotten Trump roast: Relive his brutal 2...           1\n",
       "3               Meet the happiest #dog in the world!           0\n",
       "4  Tokyos subway is shut down amid fears over an ...           1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train['postText']\n",
    "train_y = train['truthClass']\n",
    "test_x = test['postText']\n",
    "test_y = test['truthClass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2459,) (2459,) (19538,) (19538,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape, test_x.shape,test_y .shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xtest_count =  count_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train_x)\n",
    "\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train_x)\n",
    "\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(train_x)\n",
    "\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a LDA Model\n",
    "lda_model = decomposition.LatentDirichletAllocation(n_components=10, learning_method='online', max_iter=20)\n",
    "X_topics = lda_model.fit_transform(xtrain_count)\n",
    "topic_word = lda_model.components_ \n",
    "vocab = count_vect.get_feature_names()\n",
    "\n",
    "# view the topic models\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clinton hillary rally high on college video service open instagram',\n",
       " 'the a of to in rt and on as for',\n",
       " 'with and of lebron james a video after in his',\n",
       " 'the rt to in of s is for at with',\n",
       " 'to first is what police know via it like rt',\n",
       " 'the to and of is a your up for this',\n",
       " 'new gets york a state in prison reports every for',\n",
       " 'to rt in on for of year people after the',\n",
       " 'live his a report watch long questions he after hope',\n",
       " 'with to say be 2016 rt mom her run dont']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2459,) (2459,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_test, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_test)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return roc_auc_score(predictions, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.6572797349281626\n",
      "NB, WordLevel TF-IDF:  0.7362457938531912\n",
      "NB, N-Gram Vectors:  0.6784650327003756\n",
      "NB, CharLevel Vectors:  0.7563576097353598\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "auc = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count)\n",
    "print (\"NB, Count Vectors: \", auc)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "auc = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", auc)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "auc = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", auc)\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "auc = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "print (\"NB, CharLevel Vectors: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.6679062532627703\n",
      "LR, WordLevel TF-IDF:  0.7171636995117197\n",
      "LR, N-Gram Vectors:  0.689351151355335\n",
      "LR, CharLevel Vectors:  0.7254813496447201\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "auc = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xtest_count)\n",
    "print (\"LR, Count Vectors: \", auc)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "auc = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", auc)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "auc = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", auc)\n",
    "\n",
    "# Linear Classifier on Character Level TF IDF Vectors\n",
    "auc = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars)\n",
    "print (\"LR, CharLevel Vectors: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.666227494156121\n",
      "RF, WordLevel TF-IDF:  0.623548589572538\n"
     ]
    }
   ],
   "source": [
    "# RF on Count Vectors\n",
    "auc = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xtest_count)\n",
    "print (\"RF, Count Vectors: \", auc)\n",
    "\n",
    "# RF on Word Level TF IDF Vectors\n",
    "auc = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xtest_tfidf)\n",
    "print (\"RF, WordLevel TF-IDF: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, Count Vectors:  0.6941378785557214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, WordLevel TF-IDF:  0.6858915548462214\n",
      "Xgb, CharLevel Vectors:  0.6962049670767736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Extereme Gradient Boosting on Count Vectors\n",
    "auc = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xtest_count.tocsc())\n",
    "print (\"Xgb, Count Vectors: \", auc)\n",
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "auc = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xtest_tfidf.tocsc())\n",
    "print (\"Xgb, WordLevel TF-IDF: \", auc)\n",
    "\n",
    "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
    "auc = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xtest_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2459,) (2459,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As showen above, there is big change of auc score improving after tuning, now we tune XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = xgboost.XGBClassifier(\n",
    "learning_rate =0.1,\n",
    "n_estimators=1000,\n",
    "max_depth=5,\n",
    "min_child_weight=1,\n",
    "gamma=0,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'binary:logistic',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, CharLevel Vectors:  0.7006844056001513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "auc = train_model(xgb1, xtrain_tfidf_ngram_chars.tocsc(), train_y, xtest_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.72045, std: 0.02307, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.72217, std: 0.02106, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.73090, std: 0.01903, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.72657, std: 0.02703, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.73072, std: 0.01921, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.72746, std: 0.02586, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       "  mean: 0.72586, std: 0.02442, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.72067, std: 0.02129, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.71760, std: 0.02215, params: {'max_depth': 7, 'min_child_weight': 5},\n",
       "  mean: 0.72537, std: 0.02041, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.71855, std: 0.02133, params: {'max_depth': 9, 'min_child_weight': 3},\n",
       "  mean: 0.72397, std: 0.01537, params: {'max_depth': 9, 'min_child_weight': 5}],\n",
       " {'max_depth': 3, 'min_child_weight': 5},\n",
       " 0.7308978140147809)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)}\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator = xgboost.XGBClassifier(learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch1.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.72032, std: 0.02653, params: {'max_depth': 2, 'min_child_weight': 5},\n",
       "  mean: 0.71658, std: 0.02158, params: {'max_depth': 2, 'min_child_weight': 6},\n",
       "  mean: 0.71646, std: 0.02615, params: {'max_depth': 2, 'min_child_weight': 7},\n",
       "  mean: 0.73090, std: 0.01903, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.72552, std: 0.02386, params: {'max_depth': 3, 'min_child_weight': 6},\n",
       "  mean: 0.72557, std: 0.02761, params: {'max_depth': 3, 'min_child_weight': 7},\n",
       "  mean: 0.72324, std: 0.01821, params: {'max_depth': 4, 'min_child_weight': 5},\n",
       "  mean: 0.71975, std: 0.02090, params: {'max_depth': 4, 'min_child_weight': 6},\n",
       "  mean: 0.71691, std: 0.02366, params: {'max_depth': 4, 'min_child_weight': 7}],\n",
       " {'max_depth': 3, 'min_child_weight': 5},\n",
       " 0.7308978140147809)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[5,6,7]\n",
    "}\n",
    "gsearch2 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch2.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.73090, std: 0.01903, params: {'gamma': 0.0},\n",
       "  mean: 0.72500, std: 0.01631, params: {'gamma': 0.1},\n",
       "  mean: 0.72545, std: 0.01703, params: {'gamma': 0.2},\n",
       "  mean: 0.72833, std: 0.02619, params: {'gamma': 0.3},\n",
       "  mean: 0.73130, std: 0.02441, params: {'gamma': 0.4}],\n",
       " {'gamma': 0.4},\n",
       " 0.7313047863966906)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test3 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "gsearch3 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=3,\n",
    " min_child_weight=5, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch3.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2 = xgboost.XGBClassifier(\n",
    "learning_rate =0.1,\n",
    "n_estimators=1000,\n",
    "max_depth=3,\n",
    "min_child_weight=5,\n",
    "gamma=0.4,\n",
    "subsample=0.8,\n",
    "colsample_bytree=0.8,\n",
    "objective= 'binary:logistic',\n",
    "nthread=4,\n",
    "scale_pos_weight=1,\n",
    "seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.71618, std: 0.02087, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.72195, std: 0.02336, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.72846, std: 0.02557, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.71660, std: 0.02357, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.71443, std: 0.02719, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.72521, std: 0.02250, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.72965, std: 0.02475, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.71796, std: 0.02060, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.71713, std: 0.02208, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.72328, std: 0.02797, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.73203, std: 0.02490, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.71986, std: 0.02322, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.72242, std: 0.02488, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.72453, std: 0.02363, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.72384, std: 0.02007, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.72626, std: 0.02216, params: {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       " {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       " 0.7320309429195571)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "gsearch4 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=5, gamma=0.4, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch4.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.72914, std: 0.01965, params: {'colsample_bytree': 0.75, 'subsample': 0.75},\n",
       "  mean: 0.72704, std: 0.02392, params: {'colsample_bytree': 0.75, 'subsample': 0.8},\n",
       "  mean: 0.72581, std: 0.02557, params: {'colsample_bytree': 0.75, 'subsample': 0.85},\n",
       "  mean: 0.71997, std: 0.02360, params: {'colsample_bytree': 0.8, 'subsample': 0.75},\n",
       "  mean: 0.73203, std: 0.02490, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.72594, std: 0.02483, params: {'colsample_bytree': 0.8, 'subsample': 0.85},\n",
       "  mean: 0.72404, std: 0.02609, params: {'colsample_bytree': 0.85, 'subsample': 0.75},\n",
       "  mean: 0.72810, std: 0.01570, params: {'colsample_bytree': 0.85, 'subsample': 0.8},\n",
       "  mean: 0.71614, std: 0.02276, params: {'colsample_bytree': 0.85, 'subsample': 0.85}],\n",
       " {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       " 0.7320309429195571)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'subsample':[i/100.0 for i in range(75,90,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "}\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=5, gamma=0.4, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch5.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.73203, std: 0.02490, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.73398, std: 0.02347, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.72521, std: 0.01679, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.72390, std: 0.02249, params: {'reg_alpha': 1},\n",
       "  mean: 0.64926, std: 0.02571, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 0.01},\n",
       " 0.7339827091690398)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "gsearch6 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=5, gamma=0.4, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch6.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.73203, std: 0.02490, params: {'reg_alpha': 0},\n",
       "  mean: 0.73203, std: 0.02490, params: {'reg_alpha': 0.001},\n",
       "  mean: 0.73266, std: 0.02433, params: {'reg_alpha': 0.005},\n",
       "  mean: 0.73398, std: 0.02347, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.72890, std: 0.02235, params: {'reg_alpha': 0.05}],\n",
       " {'reg_alpha': 0.01},\n",
       " 0.7339827091690398)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]\n",
    "}\n",
    "gsearch7 = GridSearchCV(estimator = xgboost.XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3,\n",
    " min_child_weight=5, gamma=0.4, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), \n",
    " param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "\n",
    "gsearch7.fit(xtrain_tfidf_ngram_chars.tocsc(), train_y)\n",
    "gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3 = xgboost.XGBClassifier(\n",
    " learning_rate =0.01,\n",
    " n_estimators=1000,\n",
    " max_depth=4,\n",
    " min_child_weight=7,\n",
    " gamma=0.4,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " reg_alpha=0.01,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xgb, CharLevel Vectors:  0.7004341468668065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "auc = train_model(xgb3, xtrain_tfidf_ngram_chars.tocsc(), train_y, xtest_tfidf_ngram_chars.tocsc())\n",
    "print (\"Xgb, CharLevel Vectors: \", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
